{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "UsageError: Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ],
   "source": [
    "# set tf 1.x for colab\n",
    "%tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating names with recurrent neural networks\n",
    "\n",
    "This time you'll find yourself delving into the heart (and other intestines) of recurrent neural networks on a class of toy problems.\n",
    "\n",
    "Struggle to find a name for the variable? Let's see how you'll come up with a name for your son/daughter. Surely no human has expertize over what is a good child name, so let us train RNN instead;\n",
    "\n",
    "It's dangerous to go alone, take these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:42.696201Z",
     "start_time": "2018-08-13T20:26:38.104103Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "print(tf.__version__)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "# import keras_utils\n",
    "import tqdm_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "The dataset contains ~8k earthling names from different cultures, all in latin transcript.\n",
    "\n",
    "This notebook has been designed so as to allow you to quickly swap names for something similar: deep learning article titles, IKEA furniture, pokemon names, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:42.701832Z",
     "start_time": "2018-08-13T20:26:42.697766Z"
    }
   },
   "outputs": [],
   "source": [
    "start_token = \" \"  # so that the network knows that we're generating a first token\n",
    "\n",
    "# this is the token for padding,\n",
    "# we will add fake pad token at the end of names \n",
    "# to make them of equal size for further batching\n",
    "pad_token = \"#\"\n",
    "\n",
    "with open(\"names\") as f:\n",
    "    names = f.read()[:-1].split('\\n')\n",
    "    names = [start_token + name + pad_token for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:42.707885Z",
     "start_time": "2018-08-13T20:26:42.703302Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "number of samples: 7944\n Abagael#\n Claresta#\n Glory#\n Liliane#\n Prissie#\n Geeta#\n Giovanne#\n Piggy#\n"
     ]
    }
   ],
   "source": [
    "print('number of samples:', len(names))\n",
    "for x in names[::1000]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:42.857411Z",
     "start_time": "2018-08-13T20:26:42.709371Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "max length: 17\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"263.63625pt\" version=\"1.1\" viewBox=\"0 0 381.65 263.63625\" width=\"381.65pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M -0 263.63625 \r\nL 381.65 263.63625 \r\nL 381.65 0 \r\nL -0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 39.65 239.758125 \r\nL 374.45 239.758125 \r\nL 374.45 22.318125 \r\nL 39.65 22.318125 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path clip-path=\"url(#pc0e5dd0895)\" d=\"M 54.868182 239.758125 \r\nL 67.042727 239.758125 \r\nL 67.042727 237.837857 \r\nL 54.868182 237.837857 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path clip-path=\"url(#pc0e5dd0895)\" d=\"M 67.042727 239.758125 \r\nL 79.217273 239.758125 \r\nL 79.217273 212.267976 \r\nL 67.042727 212.267976 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path clip-path=\"url(#pc0e5dd0895)\" d=\"M 79.217273 239.758125 \r\nL 91.391818 239.758125 \r\nL 91.391818 239.758125 \r\nL 79.217273 239.758125 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path clip-path=\"url(#pc0e5dd0895)\" d=\"M 91.391818 239.758125 \r\nL 103.566364 239.758125 \r\nL 103.566364 146.17034 \r\nL 91.391818 146.17034 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_7\">\r\n    <path clip-path=\"url(#pc0e5dd0895)\" d=\"M 103.566364 239.758125 \r\nL 115.740909 239.758125 \r\nL 115.740909 239.758125 \r\nL 103.566364 239.758125 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_8\">\r\n    <path clip-path=\"url(#pc0e5dd0895)\" d=\"M 115.740909 239.758125 \r\nL 127.915455 239.758125 \r\nL 127.915455 49.95482 \r\nL 115.740909 49.95482 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_9\">\r\n    <path clip-path=\"url(#pc0e5dd0895)\" d=\"M 127.915455 239.758125 \r\nL 140.09 239.758125 \r\nL 140.09 239.758125 \r\nL 127.915455 239.758125 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_10\">\r\n    <path clip-path=\"url(#pc0e5dd0895)\" d=\"M 140.09 239.758125 \r\nL 152.264545 239.758125 \r\nL 152.264545 32.672411 \r\nL 140.09 32.672411 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_11\">\r\n    <path clip-path=\"url(#pc0e5dd0895)\" d=\"M 152.264545 239.758125 \r\nL 164.439091 239.758125 \r\nL 164.439091 239.758125 \r\nL 152.264545 239.758125 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_12\">\r\n    <path clip-path=\"url(#pc0e5dd0895)\" d=\"M 164.439091 239.758125 \r\nL 176.613636 239.758125 \r\nL 176.613636 93.514578 \r\nL 164.439091 93.514578 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_13\">\r\n    <path clip-path=\"url(#pc0e5dd0895)\" d=\"M 176.613636 239.758125 \r\nL 188.788182 239.758125 \r\nL 188.788182 239.758125 \r\nL 176.613636 239.758125 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_14\">\r\n    <path clip-path=\"url(#pc0e5dd0895)\" d=\"M 188.788182 239.758125 \r\nL 200.962727 239.758125 \r\nL 200.962727 154.255678 \r\nL 188.788182 154.255678 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_15\">\r\n    <path clip-path=\"url(#pc0e5dd0895)\" d=\"M 200.962727 239.758125 \r\nL 213.137273 239.758125 \r\nL 213.137273 239.758125 \r\nL 200.962727 239.758125 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_16\">\r\n    <path clip-path=\"url(#pc0e5dd0895)\" d=\"M 213.137273 239.758125 \r\nL 225.311818 239.758125 \r\nL 225.311818 204.283705 \r\nL 213.137273 204.283705 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_17\">\r\n    <path clip-path=\"url(#pc0e5dd0895)\" d=\"M 225.311818 239.758125 \r\nL 237.486364 239.758125 \r\nL 237.486364 239.758125 \r\nL 225.311818 239.758125 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_18\">\r\n    <path clip-path=\"url(#pc0e5dd0895)\" d=\"M 237.486364 239.758125 \r\nL 249.660909 239.758125 \r\nL 249.660909 228.034385 \r\nL 237.486364 228.034385 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_19\">\r\n    <path clip-path=\"url(#pc0e5dd0895)\" d=\"M 249.660909 239.758125 \r\nL 261.835455 239.758125 \r\nL 261.835455 239.758125 \r\nL 249.660909 239.758125 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_20\">\r\n    <path clip-path=\"url(#pc0e5dd0895)\" d=\"M 261.835455 239.758125 \r\nL 274.01 239.758125 \r\nL 274.01 237.332524 \r\nL 261.835455 237.332524 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_21\">\r\n    <path clip-path=\"url(#pc0e5dd0895)\" d=\"M 274.01 239.758125 \r\nL 286.184545 239.758125 \r\nL 286.184545 239.758125 \r\nL 274.01 239.758125 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_22\">\r\n    <path clip-path=\"url(#pc0e5dd0895)\" d=\"M 286.184545 239.758125 \r\nL 298.359091 239.758125 \r\nL 298.359091 238.747458 \r\nL 286.184545 238.747458 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_23\">\r\n    <path clip-path=\"url(#pc0e5dd0895)\" d=\"M 298.359091 239.758125 \r\nL 310.533636 239.758125 \r\nL 310.533636 239.758125 \r\nL 298.359091 239.758125 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_24\">\r\n    <path clip-path=\"url(#pc0e5dd0895)\" d=\"M 310.533636 239.758125 \r\nL 322.708182 239.758125 \r\nL 322.708182 239.454925 \r\nL 310.533636 239.454925 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_25\">\r\n    <path clip-path=\"url(#pc0e5dd0895)\" d=\"M 322.708182 239.758125 \r\nL 334.882727 239.758125 \r\nL 334.882727 239.758125 \r\nL 322.708182 239.758125 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_26\">\r\n    <path clip-path=\"url(#pc0e5dd0895)\" d=\"M 334.882727 239.758125 \r\nL 347.057273 239.758125 \r\nL 347.057273 239.657058 \r\nL 334.882727 239.657058 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_27\">\r\n    <path clip-path=\"url(#pc0e5dd0895)\" d=\"M 347.057273 239.758125 \r\nL 359.231818 239.758125 \r\nL 359.231818 239.555992 \r\nL 347.057273 239.555992 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m737f5fda55\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"54.868182\" xlink:href=\"#m737f5fda55\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 4 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(51.686932 254.356563)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"101.693357\" xlink:href=\"#m737f5fda55\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 6 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(98.512107 254.356563)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"148.518531\" xlink:href=\"#m737f5fda55\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 8 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(145.337281 254.356563)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"195.343706\" xlink:href=\"#m737f5fda55\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 10 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(188.981206 254.356563)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"242.168881\" xlink:href=\"#m737f5fda55\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 12 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(235.806381 254.356563)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"288.994056\" xlink:href=\"#m737f5fda55\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 14 -->\r\n      <g transform=\"translate(282.631556 254.356563)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"335.819231\" xlink:href=\"#m737f5fda55\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 16 -->\r\n      <g transform=\"translate(329.456731 254.356563)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_8\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m469f1eaab9\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#m469f1eaab9\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(26.2875 243.557344)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#m469f1eaab9\" y=\"214.491444\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 250 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(13.5625 218.290663)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#m469f1eaab9\" y=\"189.224764\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 500 -->\r\n      <g transform=\"translate(13.5625 193.023983)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#m469f1eaab9\" y=\"163.958083\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 750 -->\r\n      <defs>\r\n       <path d=\"M 8.203125 72.90625 \r\nL 55.078125 72.90625 \r\nL 55.078125 68.703125 \r\nL 28.609375 0 \r\nL 18.3125 0 \r\nL 43.21875 64.59375 \r\nL 8.203125 64.59375 \r\nz\r\n\" id=\"DejaVuSans-55\"/>\r\n      </defs>\r\n      <g transform=\"translate(13.5625 167.757302)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-55\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#m469f1eaab9\" y=\"138.691403\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 1000 -->\r\n      <g transform=\"translate(7.2 142.490621)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#m469f1eaab9\" y=\"113.424722\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 1250 -->\r\n      <g transform=\"translate(7.2 117.223941)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#m469f1eaab9\" y=\"88.158041\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 1500 -->\r\n      <g transform=\"translate(7.2 91.95726)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_15\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#m469f1eaab9\" y=\"62.891361\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 1750 -->\r\n      <g transform=\"translate(7.2 66.690579)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_9\">\r\n     <g id=\"line2d_16\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#m469f1eaab9\" y=\"37.62468\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_16\">\r\n      <!-- 2000 -->\r\n      <g transform=\"translate(7.2 41.423899)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_28\">\r\n    <path d=\"M 39.65 239.758125 \r\nL 39.65 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_29\">\r\n    <path d=\"M 374.45 239.758125 \r\nL 374.45 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_30\">\r\n    <path d=\"M 39.65 239.758125 \r\nL 374.45 239.758125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_31\">\r\n    <path d=\"M 39.65 22.318125 \r\nL 374.45 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"text_17\">\r\n    <!-- Sequence length distribution -->\r\n    <defs>\r\n     <path d=\"M 53.515625 70.515625 \r\nL 53.515625 60.890625 \r\nQ 47.90625 63.578125 42.921875 64.890625 \r\nQ 37.9375 66.21875 33.296875 66.21875 \r\nQ 25.25 66.21875 20.875 63.09375 \r\nQ 16.5 59.96875 16.5 54.203125 \r\nQ 16.5 49.359375 19.40625 46.890625 \r\nQ 22.3125 44.4375 30.421875 42.921875 \r\nL 36.375 41.703125 \r\nQ 47.40625 39.59375 52.65625 34.296875 \r\nQ 57.90625 29 57.90625 20.125 \r\nQ 57.90625 9.515625 50.796875 4.046875 \r\nQ 43.703125 -1.421875 29.984375 -1.421875 \r\nQ 24.8125 -1.421875 18.96875 -0.25 \r\nQ 13.140625 0.921875 6.890625 3.21875 \r\nL 6.890625 13.375 \r\nQ 12.890625 10.015625 18.65625 8.296875 \r\nQ 24.421875 6.59375 29.984375 6.59375 \r\nQ 38.421875 6.59375 43.015625 9.90625 \r\nQ 47.609375 13.234375 47.609375 19.390625 \r\nQ 47.609375 24.75 44.3125 27.78125 \r\nQ 41.015625 30.8125 33.5 32.328125 \r\nL 27.484375 33.5 \r\nQ 16.453125 35.6875 11.515625 40.375 \r\nQ 6.59375 45.0625 6.59375 53.421875 \r\nQ 6.59375 63.09375 13.40625 68.65625 \r\nQ 20.21875 74.21875 32.171875 74.21875 \r\nQ 37.3125 74.21875 42.625 73.28125 \r\nQ 47.953125 72.359375 53.515625 70.515625 \r\nz\r\n\" id=\"DejaVuSans-83\"/>\r\n     <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n     <path d=\"M 14.796875 27.296875 \r\nQ 14.796875 17.390625 18.875 11.75 \r\nQ 22.953125 6.109375 30.078125 6.109375 \r\nQ 37.203125 6.109375 41.296875 11.75 \r\nQ 45.40625 17.390625 45.40625 27.296875 \r\nQ 45.40625 37.203125 41.296875 42.84375 \r\nQ 37.203125 48.484375 30.078125 48.484375 \r\nQ 22.953125 48.484375 18.875 42.84375 \r\nQ 14.796875 37.203125 14.796875 27.296875 \r\nz\r\nM 45.40625 8.203125 \r\nQ 42.578125 3.328125 38.25 0.953125 \r\nQ 33.9375 -1.421875 27.875 -1.421875 \r\nQ 17.96875 -1.421875 11.734375 6.484375 \r\nQ 5.515625 14.40625 5.515625 27.296875 \r\nQ 5.515625 40.1875 11.734375 48.09375 \r\nQ 17.96875 56 27.875 56 \r\nQ 33.9375 56 38.25 53.625 \r\nQ 42.578125 51.265625 45.40625 46.390625 \r\nL 45.40625 54.6875 \r\nL 54.390625 54.6875 \r\nL 54.390625 -20.796875 \r\nL 45.40625 -20.796875 \r\nz\r\n\" id=\"DejaVuSans-113\"/>\r\n     <path d=\"M 8.5 21.578125 \r\nL 8.5 54.6875 \r\nL 17.484375 54.6875 \r\nL 17.484375 21.921875 \r\nQ 17.484375 14.15625 20.5 10.265625 \r\nQ 23.53125 6.390625 29.59375 6.390625 \r\nQ 36.859375 6.390625 41.078125 11.03125 \r\nQ 45.3125 15.671875 45.3125 23.6875 \r\nL 45.3125 54.6875 \r\nL 54.296875 54.6875 \r\nL 54.296875 0 \r\nL 45.3125 0 \r\nL 45.3125 8.40625 \r\nQ 42.046875 3.421875 37.71875 1 \r\nQ 33.40625 -1.421875 27.6875 -1.421875 \r\nQ 18.265625 -1.421875 13.375 4.4375 \r\nQ 8.5 10.296875 8.5 21.578125 \r\nz\r\nM 31.109375 56 \r\nz\r\n\" id=\"DejaVuSans-117\"/>\r\n     <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n     <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n     <path id=\"DejaVuSans-32\"/>\r\n     <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n     <path d=\"M 45.40625 27.984375 \r\nQ 45.40625 37.75 41.375 43.109375 \r\nQ 37.359375 48.484375 30.078125 48.484375 \r\nQ 22.859375 48.484375 18.828125 43.109375 \r\nQ 14.796875 37.75 14.796875 27.984375 \r\nQ 14.796875 18.265625 18.828125 12.890625 \r\nQ 22.859375 7.515625 30.078125 7.515625 \r\nQ 37.359375 7.515625 41.375 12.890625 \r\nQ 45.40625 18.265625 45.40625 27.984375 \r\nz\r\nM 54.390625 6.78125 \r\nQ 54.390625 -7.171875 48.1875 -13.984375 \r\nQ 42 -20.796875 29.203125 -20.796875 \r\nQ 24.46875 -20.796875 20.265625 -20.09375 \r\nQ 16.0625 -19.390625 12.109375 -17.921875 \r\nL 12.109375 -9.1875 \r\nQ 16.0625 -11.328125 19.921875 -12.34375 \r\nQ 23.78125 -13.375 27.78125 -13.375 \r\nQ 36.625 -13.375 41.015625 -8.765625 \r\nQ 45.40625 -4.15625 45.40625 5.171875 \r\nL 45.40625 9.625 \r\nQ 42.625 4.78125 38.28125 2.390625 \r\nQ 33.9375 0 27.875 0 \r\nQ 17.828125 0 11.671875 7.65625 \r\nQ 5.515625 15.328125 5.515625 27.984375 \r\nQ 5.515625 40.671875 11.671875 48.328125 \r\nQ 17.828125 56 27.875 56 \r\nQ 33.9375 56 38.28125 53.609375 \r\nQ 42.625 51.21875 45.40625 46.390625 \r\nL 45.40625 54.6875 \r\nL 54.390625 54.6875 \r\nz\r\n\" id=\"DejaVuSans-103\"/>\r\n     <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n     <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-104\"/>\r\n     <path d=\"M 45.40625 46.390625 \r\nL 45.40625 75.984375 \r\nL 54.390625 75.984375 \r\nL 54.390625 0 \r\nL 45.40625 0 \r\nL 45.40625 8.203125 \r\nQ 42.578125 3.328125 38.25 0.953125 \r\nQ 33.9375 -1.421875 27.875 -1.421875 \r\nQ 17.96875 -1.421875 11.734375 6.484375 \r\nQ 5.515625 14.40625 5.515625 27.296875 \r\nQ 5.515625 40.1875 11.734375 48.09375 \r\nQ 17.96875 56 27.875 56 \r\nQ 33.9375 56 38.25 53.625 \r\nQ 42.578125 51.265625 45.40625 46.390625 \r\nz\r\nM 14.796875 27.296875 \r\nQ 14.796875 17.390625 18.875 11.75 \r\nQ 22.953125 6.109375 30.078125 6.109375 \r\nQ 37.203125 6.109375 41.296875 11.75 \r\nQ 45.40625 17.390625 45.40625 27.296875 \r\nQ 45.40625 37.203125 41.296875 42.84375 \r\nQ 37.203125 48.484375 30.078125 48.484375 \r\nQ 22.953125 48.484375 18.875 42.84375 \r\nQ 14.796875 37.203125 14.796875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-100\"/>\r\n     <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n     <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n     <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n     <path d=\"M 48.6875 27.296875 \r\nQ 48.6875 37.203125 44.609375 42.84375 \r\nQ 40.53125 48.484375 33.40625 48.484375 \r\nQ 26.265625 48.484375 22.1875 42.84375 \r\nQ 18.109375 37.203125 18.109375 27.296875 \r\nQ 18.109375 17.390625 22.1875 11.75 \r\nQ 26.265625 6.109375 33.40625 6.109375 \r\nQ 40.53125 6.109375 44.609375 11.75 \r\nQ 48.6875 17.390625 48.6875 27.296875 \r\nz\r\nM 18.109375 46.390625 \r\nQ 20.953125 51.265625 25.265625 53.625 \r\nQ 29.59375 56 35.59375 56 \r\nQ 45.5625 56 51.78125 48.09375 \r\nQ 58.015625 40.1875 58.015625 27.296875 \r\nQ 58.015625 14.40625 51.78125 6.484375 \r\nQ 45.5625 -1.421875 35.59375 -1.421875 \r\nQ 29.59375 -1.421875 25.265625 0.953125 \r\nQ 20.953125 3.328125 18.109375 8.203125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nz\r\n\" id=\"DejaVuSans-98\"/>\r\n     <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n    </defs>\r\n    <g transform=\"translate(120.321875 16.318125)scale(0.12 -0.12)\">\r\n     <use xlink:href=\"#DejaVuSans-83\"/>\r\n     <use x=\"63.476562\" xlink:href=\"#DejaVuSans-101\"/>\r\n     <use x=\"125\" xlink:href=\"#DejaVuSans-113\"/>\r\n     <use x=\"188.476562\" xlink:href=\"#DejaVuSans-117\"/>\r\n     <use x=\"251.855469\" xlink:href=\"#DejaVuSans-101\"/>\r\n     <use x=\"313.378906\" xlink:href=\"#DejaVuSans-110\"/>\r\n     <use x=\"376.757812\" xlink:href=\"#DejaVuSans-99\"/>\r\n     <use x=\"431.738281\" xlink:href=\"#DejaVuSans-101\"/>\r\n     <use x=\"493.261719\" xlink:href=\"#DejaVuSans-32\"/>\r\n     <use x=\"525.048828\" xlink:href=\"#DejaVuSans-108\"/>\r\n     <use x=\"552.832031\" xlink:href=\"#DejaVuSans-101\"/>\r\n     <use x=\"614.355469\" xlink:href=\"#DejaVuSans-110\"/>\r\n     <use x=\"677.734375\" xlink:href=\"#DejaVuSans-103\"/>\r\n     <use x=\"741.210938\" xlink:href=\"#DejaVuSans-116\"/>\r\n     <use x=\"780.419922\" xlink:href=\"#DejaVuSans-104\"/>\r\n     <use x=\"843.798828\" xlink:href=\"#DejaVuSans-32\"/>\r\n     <use x=\"875.585938\" xlink:href=\"#DejaVuSans-100\"/>\r\n     <use x=\"939.0625\" xlink:href=\"#DejaVuSans-105\"/>\r\n     <use x=\"966.845703\" xlink:href=\"#DejaVuSans-115\"/>\r\n     <use x=\"1018.945312\" xlink:href=\"#DejaVuSans-116\"/>\r\n     <use x=\"1058.154297\" xlink:href=\"#DejaVuSans-114\"/>\r\n     <use x=\"1099.267578\" xlink:href=\"#DejaVuSans-105\"/>\r\n     <use x=\"1127.050781\" xlink:href=\"#DejaVuSans-98\"/>\r\n     <use x=\"1190.527344\" xlink:href=\"#DejaVuSans-117\"/>\r\n     <use x=\"1253.90625\" xlink:href=\"#DejaVuSans-116\"/>\r\n     <use x=\"1293.115234\" xlink:href=\"#DejaVuSans-105\"/>\r\n     <use x=\"1320.898438\" xlink:href=\"#DejaVuSans-111\"/>\r\n     <use x=\"1382.080078\" xlink:href=\"#DejaVuSans-110\"/>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pc0e5dd0895\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"39.65\" y=\"22.318125\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "MAX_LENGTH = max(map(len, names))\n",
    "print(\"max length:\", MAX_LENGTH)\n",
    "\n",
    "plt.title('Sequence length distribution')\n",
    "plt.hist(list(map(len, names)), bins=25);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing\n",
    "\n",
    "First we need to collect a \"vocabulary\" of all unique tokens i.e. unique characters. We can then encode inputs as a sequence of character ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:42.864592Z",
     "start_time": "2018-08-13T20:26:42.858725Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "n_tokens: 56\n"
     ]
    }
   ],
   "source": [
    "tokens = set([letter for name in names for letter in name]) ### YOUR CODE HERE: all unique characters go here, padding included!\n",
    "\n",
    "tokens = list(tokens)\n",
    "n_tokens = len(tokens)\n",
    "print ('n_tokens:', n_tokens)\n",
    "\n",
    "assert 50 < n_tokens < 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cast everything from symbols into identifiers\n",
    "\n",
    "Tensorflow string manipulation is a bit tricky, so we'll work around it. \n",
    "We'll feed our recurrent neural network with ids of characters from our dictionary.\n",
    "\n",
    "To create such dictionary, let's assign `token_to_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:42.870330Z",
     "start_time": "2018-08-13T20:26:42.866135Z"
    }
   },
   "outputs": [],
   "source": [
    "token_to_id = dict()\n",
    "for i,tok in enumerate(tokens):\n",
    "    token_to_id[tok] = i  ### YOUR CODE HERE: create a dictionary of {symbol -> its  index in tokens}\n",
    "\n",
    "assert len(tokens) == len(token_to_id), \"dictionaries must have same size\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:42.875943Z",
     "start_time": "2018-08-13T20:26:42.871834Z"
    }
   },
   "outputs": [],
   "source": [
    "# pad=token_to_id[pad_token]\n",
    "def to_matrix(names, max_len=None, pad=token_to_id[pad_token], dtype=np.int32):\n",
    "    \"\"\"Casts a list of names into rnn-digestable padded matrix\"\"\"\n",
    "    \n",
    "    max_len = max_len or max(map(len, names))\n",
    "    names_ix = np.zeros([len(names), max_len], dtype) + pad\n",
    "\n",
    "    for i in range(len(names)):\n",
    "        name_ix = list(map(token_to_id.get, names[i]))\n",
    "        names_ix[i, :len(name_ix)] = name_ix\n",
    "\n",
    "    return names_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:42.883107Z",
     "start_time": "2018-08-13T20:26:42.877186Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " Abagael#\n Glory#\n Prissie#\n Giovanne#\n[[51 32 34 23 49 23 46 22 47 47]\n [51 36 22 55 50  1 47 47 47 47]\n [51  3 50 25 17 17 25 46 47 47]\n [51 36 25 55 44 23  9  9 46 47]]\n"
     ]
    }
   ],
   "source": [
    "# Example: cast 4 random names to padded matrices (so that we can easily batch them)\n",
    "print('\\n'.join(names[::2000]))\n",
    "print(to_matrix(names[::2000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a recurrent neural network\n",
    "\n",
    "We can rewrite recurrent neural network as a consecutive application of dense layer to input $x_t$ and previous rnn state $h_t$. This is exactly what we're gonna do now.\n",
    "<img src=\"./rnn.png\" width=600>\n",
    "\n",
    "Since we're training a language model, there should also be:\n",
    "* An embedding layer that converts character id x_t to a vector.\n",
    "* An output layer that predicts probabilities of next phoneme based on h_t+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:44.039419Z",
     "start_time": "2018-08-13T20:26:42.884581Z"
    }
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'keras_utils' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-df6e6d1d0271>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# remember to reset your session if you change your graph!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_tf_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'keras_utils' is not defined"
     ]
    }
   ],
   "source": [
    "# remember to reset your session if you change your graph!\n",
    "s = keras_utils.reset_tf_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:44.044903Z",
     "start_time": "2018-08-13T20:26:44.041084Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import concatenate, Dense, Embedding\n",
    "\n",
    "rnn_num_units = 64  # size of hidden state\n",
    "embedding_size = 16  # for characters\n",
    "\n",
    "# Let's create layers for our recurrent network\n",
    "# Note: we create layers but we don't \"apply\" them yet (this is a \"functional API\" of Keras)\n",
    "# Note: set the correct activation (from keras.activations) to Dense layers!\n",
    "\n",
    "# an embedding layer that converts character ids into embeddings\n",
    "embed_x = Embedding(n_tokens, embedding_size)\n",
    "\n",
    "# a dense layer that maps input and previous state to new hidden state, [x_t,h_t]->h_t+1\n",
    "get_h_next = Dense(rnn_num_units, activation='relu') ### YOUR CODE HERE\n",
    "\n",
    "# a dense layer that maps current hidden state to probabilities of characters [h_t+1]->P(x_t+1|h_t+1)\n",
    "get_probas = Dense(n_tokens, activation='softmax') ### YOUR CODE HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate names character by character starting with `start_token`:\n",
    "\n",
    "<img src=\"./char-nn.png\" width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:44.053212Z",
     "start_time": "2018-08-13T20:26:44.048389Z"
    }
   },
   "outputs": [],
   "source": [
    "def rnn_one_step(x_t, h_t):\n",
    "    \"\"\"\n",
    "    Recurrent neural network step that produces \n",
    "    probabilities for next token x_t+1 and next state h_t+1\n",
    "    given current input x_t and previous state h_t.\n",
    "    We'll call this method repeatedly to produce the whole sequence.\n",
    "    \n",
    "    You're supposed to \"apply\" above layers to produce new tensors.\n",
    "    Follow inline instructions to complete the function.\n",
    "    \"\"\"\n",
    "    # convert character id into embedding\n",
    "    x_t_emb = embed_x(tf.reshape(x_t, [-1, 1]))[:, 0]\n",
    "    \n",
    "    # concatenate x_t embedding and previous h_t state\n",
    "    x_and_h = tf.concat([x_t_emb, h_t], 1) ### YOUR CODE HERE\n",
    "    \n",
    "    # compute next state given x_and_h\n",
    "    h_next = get_h_next(x_and_h) ### YOUR CODE HERE\n",
    "    \n",
    "    # get probabilities for language model P(x_next|h_next)\n",
    "    output_probas = get_probas(h_next) ### YOUR CODE HERE\n",
    "    \n",
    "    return output_probas, h_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN: loop\n",
    "\n",
    "Once `rnn_one_step` is ready, let's apply it in a loop over name characters to get predictions.\n",
    "\n",
    "Let's assume that all names are at most length-16 for now, so we can simply iterate over them in a for loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:44.342948Z",
     "start_time": "2018-08-13T20:26:44.056136Z"
    }
   },
   "outputs": [],
   "source": [
    "input_sequence = tf.placeholder(tf.int32, (None, MAX_LENGTH))  # batch of token ids\n",
    "batch_size = tf.shape(input_sequence)[0]\n",
    "\n",
    "predicted_probas = []\n",
    "h_prev = tf.zeros([batch_size, rnn_num_units])  # initial hidden state\n",
    "\n",
    "for t in range(MAX_LENGTH):\n",
    "    x_t = input_sequence[:, t]  # column t\n",
    "    probas_next, h_next = rnn_one_step(x_t, h_prev)\n",
    "    \n",
    "    h_prev = h_next\n",
    "    predicted_probas.append(probas_next)\n",
    "    \n",
    "# combine predicted_probas into [batch, time, n_tokens] tensor\n",
    "predicted_probas = tf.transpose(tf.stack(predicted_probas), [1, 0, 2])\n",
    "\n",
    "# next to last token prediction is not needed\n",
    "predicted_probas = predicted_probas[:, :-1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN: loss and gradients\n",
    "\n",
    "Let's gather a matrix of predictions for $P(x_{next}|h)$ and the corresponding correct answers.\n",
    "\n",
    "We will flatten our matrices to shape [None, n_tokens] to make it easier.\n",
    "\n",
    "Our network can then be trained by minimizing crossentropy between predicted probabilities and those answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:44.354310Z",
     "start_time": "2018-08-13T20:26:44.344648Z"
    }
   },
   "outputs": [],
   "source": [
    "# flatten predictions to [batch*time, n_tokens]\n",
    "predictions_matrix = tf.reshape(predicted_probas, [-1, n_tokens])\n",
    "\n",
    "# flatten answers (next tokens) and one-hot encode them\n",
    "answers_matrix = tf.one_hot(tf.reshape(input_sequence[:, 1:], [-1]), n_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually it's a good idea to ignore gradients of loss for padding token predictions.\n",
    "\n",
    "Because we don't care about further prediction after the pad_token is predicted for the first time, so it doesn't make sense to punish our network after the pad_token is predicted.\n",
    "\n",
    "For simplicity you can ignore this comment, it's up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:45.076642Z",
     "start_time": "2018-08-13T20:26:44.355594Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the loss as categorical cross-entropy (e.g. from keras.losses).\n",
    "# Mind that predictions are probabilities and NOT logits!\n",
    "# Remember to apply tf.reduce_mean to get a scalar loss!\n",
    "loss = tf.reduce_mean(keras.losses.categorical_crossentropy(answers_matrix, predictions_matrix)) ### YOUR CODE HERE\n",
    "\n",
    "optimize = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN: training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:55.322187Z",
     "start_time": "2018-08-13T20:26:45.078296Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 372.103125 248.518125\" width=\"372.103125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 372.103125 248.518125 \r\nL 372.103125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 364.903125 224.64 \r\nL 364.903125 7.2 \r\nL 30.103125 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"mecc4eceb51\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.321307\" xlink:href=\"#mecc4eceb51\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(42.140057 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"106.254968\" xlink:href=\"#mecc4eceb51\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 200 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(96.711218 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"167.188629\" xlink:href=\"#mecc4eceb51\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 400 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(157.644879 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"228.12229\" xlink:href=\"#mecc4eceb51\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 600 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(218.57854 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"289.055951\" xlink:href=\"#mecc4eceb51\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 800 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(279.512201 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"349.989611\" xlink:href=\"#mecc4eceb51\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 1000 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(337.264611 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m51346eac40\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m51346eac40\" y=\"206.36906\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 1.0 -->\r\n      <defs>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 210.168279)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m51346eac40\" y=\"175.076813\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 1.5 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 178.876032)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m51346eac40\" y=\"143.784566\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 2.0 -->\r\n      <g transform=\"translate(7.2 147.583784)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m51346eac40\" y=\"112.492319\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 2.5 -->\r\n      <g transform=\"translate(7.2 116.291537)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m51346eac40\" y=\"81.200071\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 3.0 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 84.99929)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m51346eac40\" y=\"49.907824\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 3.5 -->\r\n      <g transform=\"translate(7.2 53.707043)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m51346eac40\" y=\"18.615577\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 4.0 -->\r\n      <g transform=\"translate(7.2 22.414796)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_14\">\r\n    <path clip-path=\"url(#paac49ceecd)\" d=\"M 45.321307 17.083636 \r\nL 46.844648 19.274293 \r\nL 47.758653 21.276732 \r\nL 48.36799 24.07485 \r\nL 48.977326 28.378603 \r\nL 49.586663 35.586802 \r\nL 50.196 49.198346 \r\nL 50.805336 71.727429 \r\nL 52.633346 148.805749 \r\nL 52.938014 154.310794 \r\nL 53.242683 153.816093 \r\nL 53.547351 167.373659 \r\nL 53.852019 168.575749 \r\nL 54.156688 163.002574 \r\nL 54.461356 167.20046 \r\nL 54.766024 167.367243 \r\nL 55.070693 175.6817 \r\nL 55.375361 162.384929 \r\nL 55.680029 168.347931 \r\nL 55.984697 160.681602 \r\nL 56.289366 173.247729 \r\nL 56.594034 158.593664 \r\nL 56.898702 158.418711 \r\nL 57.203371 163.776296 \r\nL 57.508039 159.38509 \r\nL 57.812707 156.777793 \r\nL 58.117376 175.679171 \r\nL 58.726712 162.456357 \r\nL 59.031381 165.666229 \r\nL 59.336049 172.18302 \r\nL 59.640717 151.512534 \r\nL 59.945385 172.56352 \r\nL 60.250054 178.63647 \r\nL 60.554722 171.867143 \r\nL 61.164059 179.500548 \r\nL 61.468727 169.709918 \r\nL 61.773395 177.075925 \r\nL 62.078064 174.053838 \r\nL 62.382732 179.330833 \r\nL 62.6874 180.908828 \r\nL 62.992068 173.970174 \r\nL 63.296737 174.146813 \r\nL 63.601405 175.047418 \r\nL 63.906073 178.221672 \r\nL 64.210742 171.009951 \r\nL 64.51541 184.801312 \r\nL 64.820078 179.752688 \r\nL 65.124747 178.913215 \r\nL 65.429415 179.200793 \r\nL 66.038752 182.73195 \r\nL 66.34342 182.462151 \r\nL 66.648088 184.309446 \r\nL 66.952756 187.514147 \r\nL 67.257425 163.219418 \r\nL 67.562093 180.910745 \r\nL 67.866761 169.101606 \r\nL 68.17143 179.304482 \r\nL 68.476098 180.108829 \r\nL 68.780766 182.609581 \r\nL 69.085435 176.94875 \r\nL 69.390103 184.895614 \r\nL 69.694771 160.286158 \r\nL 69.999439 184.648271 \r\nL 70.304108 180.640878 \r\nL 70.608776 173.695637 \r\nL 70.913444 181.043948 \r\nL 71.218113 179.321999 \r\nL 71.522781 180.710501 \r\nL 71.827449 183.791647 \r\nL 72.132118 182.67795 \r\nL 72.436786 179.612293 \r\nL 72.741454 168.546354 \r\nL 73.046123 174.667947 \r\nL 73.350791 185.486468 \r\nL 73.655459 179.284636 \r\nL 73.960127 182.235489 \r\nL 74.264796 180.253939 \r\nL 74.569464 174.613283 \r\nL 74.874132 173.587629 \r\nL 75.178801 184.315728 \r\nL 75.483469 177.335384 \r\nL 75.788137 183.505105 \r\nL 76.092806 180.853589 \r\nL 76.397474 181.299602 \r\nL 76.702142 183.826152 \r\nL 77.006811 168.723813 \r\nL 77.311479 178.173096 \r\nL 77.616147 176.166628 \r\nL 77.920815 184.846926 \r\nL 78.225484 189.690293 \r\nL 78.530152 182.590854 \r\nL 78.83482 185.166458 \r\nL 79.139489 184.538033 \r\nL 79.748825 172.016184 \r\nL 80.358162 188.322091 \r\nL 80.66283 179.711915 \r\nL 80.967498 178.874733 \r\nL 81.272167 174.880023 \r\nL 81.576835 185.879824 \r\nL 81.881503 187.412152 \r\nL 82.186172 186.980136 \r\nL 82.49084 184.561228 \r\nL 82.795508 176.717306 \r\nL 83.404845 190.608498 \r\nL 83.709513 185.571773 \r\nL 84.014182 189.147426 \r\nL 84.31885 183.147382 \r\nL 84.623518 185.775666 \r\nL 84.928186 181.141697 \r\nL 85.232855 183.621588 \r\nL 85.537523 182.651301 \r\nL 85.842191 192.685558 \r\nL 86.14686 185.272944 \r\nL 86.451528 182.467097 \r\nL 86.756196 182.613766 \r\nL 87.060865 174.858037 \r\nL 87.670201 185.588978 \r\nL 87.974869 182.638752 \r\nL 88.279538 187.360331 \r\nL 88.584206 186.499513 \r\nL 88.888874 189.421336 \r\nL 89.193543 188.395989 \r\nL 89.498211 193.852464 \r\nL 89.802879 186.536481 \r\nL 90.107548 184.564443 \r\nL 90.412216 191.894923 \r\nL 90.716884 172.856963 \r\nL 91.021553 188.780645 \r\nL 91.326221 187.888612 \r\nL 91.630889 187.461631 \r\nL 91.935557 185.588739 \r\nL 92.240226 187.488646 \r\nL 92.544894 186.251312 \r\nL 92.849562 185.825615 \r\nL 93.154231 190.569956 \r\nL 93.763567 186.753257 \r\nL 94.068236 186.043145 \r\nL 94.372904 180.880171 \r\nL 94.677572 186.563428 \r\nL 94.98224 182.61452 \r\nL 95.286909 174.903129 \r\nL 95.591577 186.701368 \r\nL 95.896245 183.445293 \r\nL 96.200914 187.429745 \r\nL 96.505582 180.937029 \r\nL 96.81025 180.306306 \r\nL 97.114919 189.115434 \r\nL 97.724255 175.560435 \r\nL 98.028924 189.034934 \r\nL 98.333592 189.962278 \r\nL 98.63826 185.45905 \r\nL 98.942928 193.335225 \r\nL 99.247597 187.119449 \r\nL 99.552265 190.027469 \r\nL 99.856933 187.521496 \r\nL 100.161602 188.123033 \r\nL 100.46627 192.005512 \r\nL 100.770938 178.151557 \r\nL 101.075607 191.342566 \r\nL 101.380275 194.16799 \r\nL 101.684943 191.282142 \r\nL 101.989611 190.37527 \r\nL 102.29428 191.972969 \r\nL 102.598948 192.900925 \r\nL 103.208285 187.544295 \r\nL 103.512953 193.175268 \r\nL 103.817621 188.052254 \r\nL 104.12229 188.23166 \r\nL 104.426958 188.631223 \r\nL 104.731626 186.852827 \r\nL 105.036295 194.063623 \r\nL 105.340963 191.276457 \r\nL 105.645631 192.166207 \r\nL 105.950299 185.300168 \r\nL 106.254968 196.709126 \r\nL 106.559636 191.9182 \r\nL 106.864304 194.356319 \r\nL 107.168973 167.385462 \r\nL 107.473641 191.683123 \r\nL 107.778309 194.451905 \r\nL 108.082978 183.54267 \r\nL 108.692314 191.406385 \r\nL 108.996982 189.72388 \r\nL 109.301651 186.76941 \r\nL 109.910987 196.61504 \r\nL 110.215656 199.04143 \r\nL 110.520324 193.230164 \r\nL 110.824992 191.894065 \r\nL 111.129661 186.690185 \r\nL 111.434329 191.321967 \r\nL 111.738997 188.529482 \r\nL 112.043666 193.014894 \r\nL 112.348334 184.451482 \r\nL 112.653002 192.385379 \r\nL 112.95767 192.017613 \r\nL 113.262339 198.401597 \r\nL 113.567007 190.064056 \r\nL 113.871675 187.011694 \r\nL 114.176344 186.145222 \r\nL 114.481012 190.347114 \r\nL 115.090349 194.950128 \r\nL 115.395017 187.838081 \r\nL 115.699685 185.975633 \r\nL 116.004354 194.020456 \r\nL 116.309022 196.696413 \r\nL 116.61369 191.729692 \r\nL 116.918358 191.050668 \r\nL 117.223027 193.519204 \r\nL 117.527695 191.826442 \r\nL 117.832363 194.455113 \r\nL 118.137032 188.542292 \r\nL 118.4417 187.297907 \r\nL 118.746368 192.758419 \r\nL 119.051037 189.099931 \r\nL 119.355705 192.242798 \r\nL 119.660373 198.349828 \r\nL 119.965041 194.775795 \r\nL 120.26971 186.121773 \r\nL 120.574378 198.96851 \r\nL 120.879046 194.274222 \r\nL 121.183715 192.91557 \r\nL 121.488383 198.474301 \r\nL 121.793051 197.612529 \r\nL 122.402388 187.493085 \r\nL 122.707056 197.008231 \r\nL 123.316393 189.502553 \r\nL 123.621061 196.202607 \r\nL 123.925729 191.44462 \r\nL 124.230398 189.744121 \r\nL 124.535066 195.368476 \r\nL 124.839734 196.255831 \r\nL 125.144403 185.401432 \r\nL 125.449071 193.502642 \r\nL 125.753739 186.93379 \r\nL 126.058408 200.004705 \r\nL 126.363076 202.068874 \r\nL 126.667744 198.809822 \r\nL 126.972412 189.314395 \r\nL 127.277081 194.731202 \r\nL 127.581749 193.089336 \r\nL 127.886417 190.009146 \r\nL 128.191086 199.300852 \r\nL 128.495754 187.700447 \r\nL 128.800422 199.664977 \r\nL 129.105091 198.765356 \r\nL 129.409759 189.096663 \r\nL 129.714427 194.592263 \r\nL 130.019096 192.411819 \r\nL 130.323764 188.237748 \r\nL 130.628432 196.486291 \r\nL 130.9331 197.649705 \r\nL 131.237769 198.322828 \r\nL 131.542437 193.266788 \r\nL 131.847105 191.351176 \r\nL 132.151774 193.465696 \r\nL 132.456442 193.049153 \r\nL 132.76111 188.940669 \r\nL 133.065779 196.443773 \r\nL 133.370447 195.062798 \r\nL 133.979783 196.505114 \r\nL 134.284452 195.613977 \r\nL 134.58912 198.824072 \r\nL 134.893788 198.04127 \r\nL 135.198457 204.685138 \r\nL 135.503125 193.117209 \r\nL 136.112462 204.707864 \r\nL 136.41713 191.294512 \r\nL 137.026467 203.644094 \r\nL 137.331135 197.032821 \r\nL 137.635803 199.511944 \r\nL 137.940471 194.807734 \r\nL 138.24514 194.679843 \r\nL 138.549808 192.863681 \r\nL 138.854476 193.759094 \r\nL 139.159145 196.658304 \r\nL 139.463813 201.914856 \r\nL 139.768481 198.490103 \r\nL 140.07315 193.705168 \r\nL 140.377818 202.422121 \r\nL 140.682486 196.74061 \r\nL 140.987154 201.178878 \r\nL 141.596491 193.911866 \r\nL 141.901159 199.873995 \r\nL 142.205828 192.167162 \r\nL 142.510496 194.994936 \r\nL 142.815164 200.010614 \r\nL 143.119833 201.913782 \r\nL 143.424501 194.733731 \r\nL 143.729169 195.980086 \r\nL 144.033838 199.599375 \r\nL 144.338506 199.281253 \r\nL 144.643174 198.387422 \r\nL 144.947842 204.702141 \r\nL 145.252511 201.550254 \r\nL 145.557179 190.252833 \r\nL 145.861847 199.511556 \r\nL 146.166516 193.039574 \r\nL 146.471184 197.436965 \r\nL 146.775852 204.061838 \r\nL 147.385189 184.656776 \r\nL 147.689857 198.266358 \r\nL 147.994525 193.456788 \r\nL 148.299194 195.840437 \r\nL 148.603862 195.868206 \r\nL 148.90853 201.313117 \r\nL 149.213199 200.106767 \r\nL 149.517867 201.933418 \r\nL 149.822535 198.891703 \r\nL 150.127204 194.25281 \r\nL 150.431872 199.605501 \r\nL 150.73654 195.430683 \r\nL 151.041209 203.865146 \r\nL 151.345877 194.28986 \r\nL 151.650545 200.005347 \r\nL 151.955213 202.317299 \r\nL 152.259882 195.581553 \r\nL 152.56455 199.038409 \r\nL 152.869218 193.382943 \r\nL 153.478555 198.746794 \r\nL 153.783223 198.688333 \r\nL 154.087892 197.418529 \r\nL 154.39256 199.260259 \r\nL 154.697228 198.740662 \r\nL 155.001896 196.072792 \r\nL 155.306565 197.593609 \r\nL 155.611233 201.182429 \r\nL 155.915901 201.179452 \r\nL 156.22057 200.748652 \r\nL 156.525238 206.624811 \r\nL 157.134575 195.966709 \r\nL 157.439243 203.372235 \r\nL 157.743911 198.322858 \r\nL 158.04858 200.829749 \r\nL 158.353248 198.918046 \r\nL 158.657916 196.183993 \r\nL 158.962584 197.519278 \r\nL 159.267253 189.986749 \r\nL 159.876589 202.481635 \r\nL 160.485926 200.974247 \r\nL 160.790594 206.067239 \r\nL 161.095263 189.92308 \r\nL 161.399931 200.876632 \r\nL 161.704599 202.235008 \r\nL 162.009268 198.97145 \r\nL 162.313936 198.888741 \r\nL 162.618604 200.417332 \r\nL 162.923272 193.382376 \r\nL 163.227941 203.420513 \r\nL 163.532609 200.135968 \r\nL 163.837277 192.774027 \r\nL 164.141946 195.222464 \r\nL 164.446614 195.664172 \r\nL 164.751282 194.146593 \r\nL 165.360619 199.191501 \r\nL 165.665287 200.536448 \r\nL 165.969955 191.452984 \r\nL 166.274624 195.971849 \r\nL 166.579292 207.22097 \r\nL 166.88396 198.805353 \r\nL 167.188629 198.125583 \r\nL 167.493297 199.670595 \r\nL 167.797965 193.772284 \r\nL 168.102634 199.203364 \r\nL 168.407302 200.220587 \r\nL 168.71197 198.228115 \r\nL 169.016639 195.435279 \r\nL 169.321307 196.86753 \r\nL 169.625975 200.448435 \r\nL 169.930643 196.738178 \r\nL 170.235312 196.201481 \r\nL 170.53998 200.499347 \r\nL 170.844648 192.668347 \r\nL 171.149317 200.263612 \r\nL 171.453985 199.446059 \r\nL 171.758653 200.746637 \r\nL 172.063322 188.855005 \r\nL 172.36799 199.545644 \r\nL 172.977326 196.386938 \r\nL 173.281995 203.687463 \r\nL 173.586663 198.646247 \r\nL 173.891331 198.124815 \r\nL 174.196 205.586079 \r\nL 174.500668 201.675936 \r\nL 174.805336 199.500649 \r\nL 175.110005 204.852794 \r\nL 175.719341 196.479576 \r\nL 176.02401 206.020909 \r\nL 176.633346 192.751137 \r\nL 176.938014 202.325849 \r\nL 177.242683 202.361264 \r\nL 177.547351 200.775973 \r\nL 177.852019 201.181086 \r\nL 178.461356 194.577923 \r\nL 178.766024 199.445141 \r\nL 179.070693 194.21776 \r\nL 179.375361 192.784285 \r\nL 179.984697 203.862161 \r\nL 180.289366 199.826604 \r\nL 180.594034 198.763566 \r\nL 180.898702 201.770821 \r\nL 181.203371 198.626424 \r\nL 181.508039 203.282372 \r\nL 181.812707 201.541734 \r\nL 182.117376 196.701173 \r\nL 182.422044 198.706619 \r\nL 182.726712 193.703079 \r\nL 183.031381 201.595496 \r\nL 183.336049 197.983972 \r\nL 183.640717 195.966104 \r\nL 183.945385 201.826835 \r\nL 184.250054 198.627625 \r\nL 184.554722 201.607552 \r\nL 184.85939 201.249851 \r\nL 185.164059 201.728855 \r\nL 185.468727 202.824802 \r\nL 185.773395 195.186145 \r\nL 186.078064 196.409775 \r\nL 186.382732 204.585233 \r\nL 186.6874 198.382729 \r\nL 186.992068 206.191698 \r\nL 187.296737 202.696673 \r\nL 187.601405 195.240922 \r\nL 187.906073 202.815864 \r\nL 188.210742 201.423915 \r\nL 188.820078 204.306115 \r\nL 189.124747 198.599364 \r\nL 189.429415 204.27528 \r\nL 189.734083 205.803461 \r\nL 190.34342 196.547371 \r\nL 190.648088 201.398997 \r\nL 190.952756 196.378984 \r\nL 191.257425 201.025084 \r\nL 191.562093 202.433536 \r\nL 191.866761 198.840246 \r\nL 192.17143 197.712166 \r\nL 192.476098 203.388261 \r\nL 192.780766 203.229259 \r\nL 193.085435 198.657834 \r\nL 193.390103 198.193348 \r\nL 193.694771 201.455377 \r\nL 193.999439 200.83847 \r\nL 194.304108 201.385762 \r\nL 194.608776 206.207045 \r\nL 194.913444 206.743872 \r\nL 195.218113 203.191046 \r\nL 195.522781 205.081471 \r\nL 195.827449 202.813193 \r\nL 196.132118 205.084485 \r\nL 196.436786 205.633671 \r\nL 196.741454 198.323298 \r\nL 197.046123 202.597237 \r\nL 197.350791 199.95398 \r\nL 197.655459 201.202968 \r\nL 197.960127 201.707711 \r\nL 198.264796 198.942495 \r\nL 198.874132 204.095381 \r\nL 199.178801 202.805792 \r\nL 199.483469 197.564057 \r\nL 199.788137 201.617579 \r\nL 200.092806 200.426143 \r\nL 200.397474 204.665032 \r\nL 200.702142 203.540048 \r\nL 201.006811 202.903676 \r\nL 201.311479 197.935874 \r\nL 201.616147 199.178117 \r\nL 201.920815 207.893809 \r\nL 202.225484 197.007835 \r\nL 203.139489 208.012993 \r\nL 203.444157 204.283285 \r\nL 203.748825 203.79612 \r\nL 204.053494 205.796962 \r\nL 204.358162 201.172171 \r\nL 204.66283 204.683788 \r\nL 205.272167 197.954652 \r\nL 205.576835 200.796758 \r\nL 205.881503 201.01687 \r\nL 206.186172 204.086145 \r\nL 206.49084 202.898431 \r\nL 206.795508 206.321871 \r\nL 207.100177 203.52735 \r\nL 207.709513 206.093516 \r\nL 208.31885 195.899555 \r\nL 208.623518 203.981338 \r\nL 208.928186 199.00588 \r\nL 209.232855 201.271897 \r\nL 209.537523 198.118525 \r\nL 209.842191 199.510303 \r\nL 210.14686 199.931889 \r\nL 210.451528 197.680936 \r\nL 210.756196 196.269067 \r\nL 211.060865 203.366535 \r\nL 211.365533 203.026389 \r\nL 211.670201 205.798648 \r\nL 212.279538 199.886573 \r\nL 212.584206 199.409106 \r\nL 212.888874 197.245711 \r\nL 213.193543 202.500309 \r\nL 213.498211 199.177923 \r\nL 213.802879 199.688911 \r\nL 214.107548 199.414702 \r\nL 214.412216 205.237727 \r\nL 214.716884 208.27341 \r\nL 215.021553 194.687304 \r\nL 215.326221 209.168237 \r\nL 215.630889 200.999359 \r\nL 215.935557 199.072586 \r\nL 216.240226 204.755701 \r\nL 216.544894 194.629334 \r\nL 216.849562 200.367785 \r\nL 217.154231 203.912222 \r\nL 217.458899 202.671068 \r\nL 217.763567 204.946836 \r\nL 218.068236 197.966686 \r\nL 218.372904 201.18798 \r\nL 218.677572 199.610917 \r\nL 218.98224 204.147084 \r\nL 219.286909 197.691664 \r\nL 219.896245 203.725549 \r\nL 220.200914 201.3273 \r\nL 220.505582 202.625513 \r\nL 220.81025 200.485059 \r\nL 221.114919 205.745073 \r\nL 221.419587 193.887164 \r\nL 221.724255 205.35712 \r\nL 222.028924 202.76843 \r\nL 222.333592 199.116723 \r\nL 222.63826 202.176143 \r\nL 222.942928 199.299644 \r\nL 223.247597 204.611554 \r\nL 223.552265 204.198331 \r\nL 223.856933 201.452124 \r\nL 224.161602 201.812153 \r\nL 224.770938 204.511865 \r\nL 225.075607 202.853996 \r\nL 225.380275 209.394851 \r\nL 225.684943 207.58743 \r\nL 225.989611 208.941781 \r\nL 226.598948 203.879022 \r\nL 226.903616 194.383387 \r\nL 227.208285 203.70031 \r\nL 227.512953 205.589258 \r\nL 227.817621 202.977097 \r\nL 228.12229 198.430381 \r\nL 228.426958 200.2085 \r\nL 228.731626 204.140817 \r\nL 229.036295 201.778282 \r\nL 229.340963 203.21007 \r\nL 229.645631 202.169914 \r\nL 229.950299 203.549679 \r\nL 230.254968 207.05854 \r\nL 230.559636 202.562254 \r\nL 230.864304 203.215815 \r\nL 231.168973 205.919578 \r\nL 231.473641 205.592302 \r\nL 231.778309 201.033082 \r\nL 232.082978 194.744982 \r\nL 232.387646 207.560612 \r\nL 232.692314 206.793997 \r\nL 232.996982 199.356539 \r\nL 233.301651 202.570618 \r\nL 233.606319 194.851871 \r\nL 233.910987 205.278857 \r\nL 234.215656 195.4465 \r\nL 234.520324 205.114305 \r\nL 234.824992 200.354058 \r\nL 235.434329 204.41457 \r\nL 235.738997 208.729946 \r\nL 236.043666 201.82212 \r\nL 236.348334 202.069441 \r\nL 236.653002 210.762825 \r\nL 236.95767 203.792732 \r\nL 237.262339 199.937022 \r\nL 237.567007 198.609809 \r\nL 237.871675 206.63648 \r\nL 238.176344 204.14454 \r\nL 238.481012 203.074779 \r\nL 238.78568 202.810687 \r\nL 239.090349 205.306723 \r\nL 239.395017 199.870839 \r\nL 239.699685 203.870965 \r\nL 240.309022 205.505012 \r\nL 240.61369 207.202766 \r\nL 240.918358 200.673911 \r\nL 241.223027 203.698728 \r\nL 241.527695 203.849695 \r\nL 241.832363 203.340184 \r\nL 242.137032 207.663379 \r\nL 242.4417 200.285741 \r\nL 242.746368 200.256099 \r\nL 243.051037 203.743298 \r\nL 243.355705 204.942031 \r\nL 243.660373 203.889863 \r\nL 243.965041 205.635327 \r\nL 244.26971 205.860922 \r\nL 244.574378 201.947661 \r\nL 244.879046 204.318238 \r\nL 245.183715 202.971158 \r\nL 245.488383 203.900427 \r\nL 245.793051 199.967797 \r\nL 246.09772 201.061149 \r\nL 246.402388 201.624167 \r\nL 246.707056 201.781281 \r\nL 247.011725 201.512921 \r\nL 247.316393 205.675219 \r\nL 247.621061 202.62476 \r\nL 247.925729 206.392494 \r\nL 248.230398 208.488445 \r\nL 248.535066 196.713475 \r\nL 248.839734 201.71318 \r\nL 249.144403 199.503245 \r\nL 249.449071 202.73232 \r\nL 249.753739 203.744044 \r\nL 250.058408 199.145268 \r\nL 250.363076 208.069552 \r\nL 250.667744 206.765049 \r\nL 250.972412 207.490038 \r\nL 251.277081 207.628694 \r\nL 251.581749 197.476812 \r\nL 251.886417 201.834572 \r\nL 252.191086 201.044653 \r\nL 252.495754 201.868854 \r\nL 253.105091 206.59044 \r\nL 253.409759 206.393788 \r\nL 253.714427 201.664999 \r\nL 254.019096 211.051437 \r\nL 254.323764 202.958243 \r\nL 254.628432 201.242234 \r\nL 254.9331 203.894347 \r\nL 255.237769 205.282446 \r\nL 255.542437 202.046955 \r\nL 255.847105 204.168526 \r\nL 256.151774 202.331981 \r\nL 256.456442 204.053781 \r\nL 256.76111 201.942796 \r\nL 257.065779 204.441608 \r\nL 257.370447 197.898966 \r\nL 257.675115 203.466836 \r\nL 257.979783 201.977391 \r\nL 258.284452 202.17145 \r\nL 258.58912 206.669821 \r\nL 258.893788 205.027471 \r\nL 259.503125 205.643795 \r\nL 259.807793 203.062297 \r\nL 260.112462 202.692995 \r\nL 260.41713 200.974098 \r\nL 261.026467 206.005174 \r\nL 261.331135 198.429672 \r\nL 261.635803 201.005067 \r\nL 261.940471 201.960859 \r\nL 262.24514 200.592142 \r\nL 262.854476 204.575012 \r\nL 263.159145 199.974139 \r\nL 263.768481 206.768787 \r\nL 264.07315 202.927871 \r\nL 264.377818 206.433699 \r\nL 264.682486 208.096 \r\nL 264.987154 206.144867 \r\nL 265.291823 200.586614 \r\nL 265.596491 209.471021 \r\nL 266.205828 197.392537 \r\nL 266.510496 203.59607 \r\nL 266.815164 207.371011 \r\nL 267.119833 200.067748 \r\nL 267.424501 207.140365 \r\nL 267.729169 201.52735 \r\nL 268.033838 205.99646 \r\nL 268.338506 206.728656 \r\nL 268.643174 197.755303 \r\nL 268.947842 207.934794 \r\nL 269.252511 206.646925 \r\nL 269.557179 201.194657 \r\nL 269.861847 210.45561 \r\nL 270.471184 203.261892 \r\nL 270.775852 201.104182 \r\nL 271.080521 206.312434 \r\nL 271.385189 200.598722 \r\nL 271.689857 208.791482 \r\nL 271.994525 205.813973 \r\nL 272.299194 205.430197 \r\nL 272.603862 205.907156 \r\nL 272.90853 207.554838 \r\nL 273.213199 205.165739 \r\nL 273.517867 207.495473 \r\nL 273.822535 203.948794 \r\nL 274.127204 205.253603 \r\nL 274.431872 205.728026 \r\nL 274.73654 206.5252 \r\nL 275.041209 201.835908 \r\nL 275.345877 206.118979 \r\nL 275.650545 200.670718 \r\nL 275.955213 209.262633 \r\nL 276.259882 205.235235 \r\nL 276.56455 197.871779 \r\nL 276.869218 207.434139 \r\nL 277.173887 204.700843 \r\nL 277.783223 203.64331 \r\nL 278.087892 206.525633 \r\nL 278.39256 203.304627 \r\nL 278.697228 204.522086 \r\nL 279.001896 204.435766 \r\nL 279.306565 200.403141 \r\nL 279.611233 207.091423 \r\nL 279.915901 203.775021 \r\nL 280.22057 203.423259 \r\nL 280.525238 211.478794 \r\nL 280.829906 203.246516 \r\nL 281.134575 201.585498 \r\nL 281.439243 207.383552 \r\nL 281.743911 203.173647 \r\nL 282.04858 204.850086 \r\nL 282.353248 195.423193 \r\nL 282.657916 207.143122 \r\nL 282.962584 202.753195 \r\nL 283.267253 206.978021 \r\nL 283.571921 196.816679 \r\nL 284.181258 204.811604 \r\nL 284.485926 202.623171 \r\nL 284.790594 206.570755 \r\nL 285.095263 201.240174 \r\nL 285.399931 206.173285 \r\nL 285.704599 207.272915 \r\nL 286.009268 199.523881 \r\nL 286.313936 206.287649 \r\nL 286.618604 202.552175 \r\nL 286.923272 207.775352 \r\nL 287.227941 195.824568 \r\nL 287.532609 209.25067 \r\nL 287.837277 205.033469 \r\nL 288.141946 203.961754 \r\nL 288.751282 209.480298 \r\nL 289.055951 203.128764 \r\nL 289.360619 202.438184 \r\nL 289.665287 207.934219 \r\nL 290.274624 206.339605 \r\nL 290.579292 210.603324 \r\nL 290.88396 201.861214 \r\nL 291.188629 207.884401 \r\nL 291.493297 205.897763 \r\nL 291.797965 206.025542 \r\nL 292.102634 205.08065 \r\nL 292.407302 206.713265 \r\nL 292.71197 200.356923 \r\nL 293.016639 209.880424 \r\nL 293.321307 198.27787 \r\nL 293.625975 207.858889 \r\nL 293.930643 208.798827 \r\nL 294.235312 202.44119 \r\nL 294.53998 211.530996 \r\nL 294.844648 203.640483 \r\nL 295.149317 204.860247 \r\nL 295.453985 202.947067 \r\nL 295.758653 204.69583 \r\nL 296.063322 209.233138 \r\nL 296.36799 203.050144 \r\nL 296.672658 205.659821 \r\nL 296.977326 196.180046 \r\nL 297.281995 203.803774 \r\nL 297.586663 206.137467 \r\nL 297.891331 202.784321 \r\nL 298.196 202.433461 \r\nL 298.500668 207.223868 \r\nL 298.805336 201.108472 \r\nL 299.110005 208.589806 \r\nL 299.414673 199.68571 \r\nL 299.719341 203.88608 \r\nL 300.02401 198.599133 \r\nL 300.328678 200.251235 \r\nL 300.633346 205.996445 \r\nL 300.938014 205.728533 \r\nL 301.242683 206.446629 \r\nL 301.547351 206.351557 \r\nL 301.852019 206.086525 \r\nL 302.156688 205.315131 \r\nL 302.461356 205.670624 \r\nL 302.766024 204.20274 \r\nL 303.070693 204.904952 \r\nL 303.375361 206.327101 \r\nL 303.680029 203.658754 \r\nL 303.984697 204.830181 \r\nL 304.289366 202.497847 \r\nL 304.594034 202.819378 \r\nL 304.898702 203.631911 \r\nL 305.203371 210.228087 \r\nL 306.117376 202.896887 \r\nL 306.422044 202.634406 \r\nL 306.726712 213.969071 \r\nL 307.031381 202.352729 \r\nL 307.640717 211.315234 \r\nL 307.945385 207.10928 \r\nL 308.250054 208.989861 \r\nL 308.554722 203.302866 \r\nL 308.85939 201.77473 \r\nL 309.164059 206.163728 \r\nL 309.468727 204.719435 \r\nL 309.773395 206.689577 \r\nL 310.078064 201.813846 \r\nL 310.6874 209.151615 \r\nL 310.992068 206.122306 \r\nL 311.296737 208.501736 \r\nL 311.601405 204.177829 \r\nL 311.906073 204.119233 \r\nL 312.210742 208.826864 \r\nL 312.51541 203.703898 \r\nL 312.820078 204.87211 \r\nL 313.124747 206.875034 \r\nL 313.429415 202.014598 \r\nL 314.038752 209.542505 \r\nL 314.34342 206.666904 \r\nL 314.648088 207.592682 \r\nL 314.952756 209.249786 \r\nL 315.257425 203.710001 \r\nL 315.562093 205.662991 \r\nL 315.866761 209.248566 \r\nL 316.17143 209.817273 \r\nL 316.780766 204.61612 \r\nL 317.085435 205.462718 \r\nL 317.390103 201.882126 \r\nL 317.694771 205.555789 \r\nL 317.999439 205.259728 \r\nL 318.304108 204.020626 \r\nL 318.608776 201.134173 \r\nL 318.913444 207.461132 \r\nL 319.218113 201.434107 \r\nL 319.522781 207.184457 \r\nL 319.827449 208.963756 \r\nL 320.132118 207.210633 \r\nL 320.436786 208.038277 \r\nL 320.741454 209.984951 \r\nL 321.046123 208.028059 \r\nL 321.350791 209.946936 \r\nL 321.655459 202.181769 \r\nL 321.960127 207.347118 \r\nL 322.264796 209.172714 \r\nL 322.569464 203.904374 \r\nL 322.874132 205.357948 \r\nL 323.178801 197.965403 \r\nL 323.483469 200.192497 \r\nL 323.788137 204.449829 \r\nL 324.092806 203.02427 \r\nL 324.397474 209.869972 \r\nL 324.702142 205.476878 \r\nL 325.006811 207.641411 \r\nL 325.311479 207.822313 \r\nL 325.616147 202.202897 \r\nL 325.920815 207.316015 \r\nL 326.225484 198.770736 \r\nL 326.530152 205.774938 \r\nL 326.83482 205.805288 \r\nL 327.139489 203.520799 \r\nL 327.444157 210.30796 \r\nL 327.748825 202.988295 \r\nL 328.053494 209.166794 \r\nL 328.358162 208.051505 \r\nL 328.66283 203.500648 \r\nL 328.967498 204.751934 \r\nL 329.272167 199.748663 \r\nL 329.576835 200.840194 \r\nL 329.881503 199.897555 \r\nL 330.49084 207.794261 \r\nL 330.795508 208.064956 \r\nL 331.100177 204.530748 \r\nL 331.404845 204.698866 \r\nL 331.709513 208.634965 \r\nL 332.014182 204.491363 \r\nL 332.31885 204.12882 \r\nL 332.623518 209.215542 \r\nL 332.928186 205.406628 \r\nL 333.232855 204.513999 \r\nL 333.537523 208.297676 \r\nL 333.842191 208.491176 \r\nL 334.14686 211.749168 \r\nL 334.451528 207.639524 \r\nL 334.756196 207.13656 \r\nL 335.060865 200.561419 \r\nL 335.365533 203.081359 \r\nL 335.670201 214.756364 \r\nL 335.974869 197.289027 \r\nL 336.279538 201.90462 \r\nL 336.584206 202.126023 \r\nL 336.888874 207.790505 \r\nL 337.193543 207.231303 \r\nL 337.498211 210.750034 \r\nL 337.802879 202.020178 \r\nL 338.107548 204.132177 \r\nL 338.412216 210.485822 \r\nL 338.716884 212.284784 \r\nL 339.021553 204.282815 \r\nL 339.326221 209.57203 \r\nL 339.630889 204.863933 \r\nL 339.935557 204.062084 \r\nL 340.240226 202.872401 \r\nL 340.544894 211.016842 \r\nL 340.849562 206.146181 \r\nL 341.154231 210.878697 \r\nL 341.458899 208.228512 \r\nL 341.763567 208.344906 \r\nL 342.068236 205.455764 \r\nL 342.372904 199.975579 \r\nL 342.677572 203.880276 \r\nL 342.98224 201.533072 \r\nL 343.286909 207.079844 \r\nL 343.591577 202.163221 \r\nL 343.896245 204.195787 \r\nL 344.200914 203.812496 \r\nL 344.505582 210.806925 \r\nL 344.81025 207.394087 \r\nL 345.114919 205.387887 \r\nL 345.724255 208.628489 \r\nL 346.028924 202.703238 \r\nL 346.333592 207.5029 \r\nL 346.63826 201.952189 \r\nL 346.942928 208.092713 \r\nL 347.247597 205.848299 \r\nL 347.552265 208.141581 \r\nL 347.856933 214.141278 \r\nL 348.161602 197.809662 \r\nL 348.46627 206.754824 \r\nL 348.770938 205.638006 \r\nL 349.075607 199.956532 \r\nL 349.380275 208.890697 \r\nL 349.684943 196.098904 \r\nL 349.684943 196.098904 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 30.103125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 364.903125 224.64 \r\nL 364.903125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 364.903125 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 30.103125 7.2 \r\nL 364.903125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 306.5875 29.878125 \r\nL 357.903125 29.878125 \r\nQ 359.903125 29.878125 359.903125 27.878125 \r\nL 359.903125 14.2 \r\nQ 359.903125 12.2 357.903125 12.2 \r\nL 306.5875 12.2 \r\nQ 304.5875 12.2 304.5875 14.2 \r\nL 304.5875 27.878125 \r\nQ 304.5875 29.878125 306.5875 29.878125 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_15\">\r\n     <path d=\"M 308.5875 20.298437 \r\nL 328.5875 20.298437 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_16\"/>\r\n    <g id=\"text_14\">\r\n     <!-- loss -->\r\n     <defs>\r\n      <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n      <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n      <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n     </defs>\r\n     <g transform=\"translate(336.5875 23.798437)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"paac49ceecd\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"30.103125\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from random import sample\n",
    "\n",
    "s = tf.Session()\n",
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "batch_size = 32\n",
    "history = []\n",
    "\n",
    "for i in range(1000):\n",
    "    batch = to_matrix(sample(names, batch_size), max_len=MAX_LENGTH)\n",
    "    loss_i, _ = s.run([loss, optimize], {input_sequence: batch})\n",
    "    \n",
    "    history.append(loss_i)\n",
    "    \n",
    "    if (i + 1) % 100 == 0:\n",
    "        clear_output(True)\n",
    "        plt.plot(history, label='loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "assert np.mean(history[:10]) > np.mean(history[-10:]), \"RNN didn't converge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN: sampling\n",
    "Once we've trained our network a bit, let's get to actually generating stuff. All we need is the `rnn_one_step` function you have written above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:55.341196Z",
     "start_time": "2018-08-13T20:26:55.323787Z"
    }
   },
   "outputs": [],
   "source": [
    "x_t = tf.placeholder(tf.int32, (1,))\n",
    "h_t = tf.Variable(np.zeros([1, rnn_num_units], np.float32))  # we will update hidden state in this variable\n",
    "\n",
    "# For sampling we need to define `rnn_one_step` tensors only once in our graph.\n",
    "# We reuse all parameters thanks to functional API usage.\n",
    "# Then we can feed appropriate tensor values using feed_dict in a loop.\n",
    "# Note how different it is from training stage, where we had to unroll the whole sequence for backprop.\n",
    "next_probs, next_h = rnn_one_step(x_t, h_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:55.346422Z",
     "start_time": "2018-08-13T20:26:55.342659Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_sample(seed_phrase=start_token, max_length=MAX_LENGTH):\n",
    "    '''\n",
    "    This function generates text given a `seed_phrase` as a seed.\n",
    "    Remember to include start_token in seed phrase!\n",
    "    Parameter `max_length` is used to set the number of characters in prediction.\n",
    "    '''\n",
    "    x_sequence = [token_to_id[token] for token in seed_phrase]\n",
    "    s.run(tf.assign(h_t, h_t.initial_value))\n",
    "    \n",
    "    # feed the seed phrase, if any\n",
    "    for ix in x_sequence[:-1]:\n",
    "         s.run(tf.assign(h_t, next_h), {x_t: [ix]})\n",
    "    \n",
    "    # start generating\n",
    "    for _ in range(max_length-len(seed_phrase)):\n",
    "        x_probs,_ = s.run([next_probs, tf.assign(h_t, next_h)], {x_t: [x_sequence[-1]]})\n",
    "        x_sequence.append(np.random.choice(n_tokens, p=x_probs[0]))\n",
    "        \n",
    "    return ''.join([tokens[ix] for ix in x_sequence if tokens[ix] != pad_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:58.458115Z",
     "start_time": "2018-08-13T20:26:55.347900Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " Mellalice\n",
      " Nernee\n",
      " Flhad\n",
      " Rory\n",
      " Maeron\n",
      " Vet\n",
      " Jegalpen\n",
      " Jabes\n",
      " Davi\n",
      " Katt\n"
     ]
    }
   ],
   "source": [
    "# without prefix\n",
    "for _ in range(10):\n",
    "    print(generate_sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:27:01.986726Z",
     "start_time": "2018-08-13T20:26:58.459810Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " Trumptoa\n",
      " Trumpyte\n",
      " Trumpy\n",
      " Trumpea\n",
      " Trumpesare\n",
      " Trumpiinal\n",
      " Trumpa\n",
      " Trumpiad\n",
      " Trumpon\n",
      " Trumpine\n"
     ]
    }
   ],
   "source": [
    "# with prefix conditioning\n",
    "for _ in range(10):\n",
    "    print(generate_sample(' Trump'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit to Coursera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:40:02.004926Z",
     "start_time": "2018-08-13T20:40:02.000821Z"
    }
   },
   "outputs": [],
   "source": [
    "# token expires every 30 min\n",
    "COURSERA_TOKEN = \"bow7MCMJaoJakU8w\"\n",
    "COURSERA_EMAIL = \"A01638188@itesm.mx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:40:18.923357Z",
     "start_time": "2018-08-13T20:40:03.549343Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d55fab4aeb40466cb93ea22ebdfb4c9d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Submitted to Coursera platform. See results on assignment page!\n"
     ]
    }
   ],
   "source": [
    "from submit import submit_char_rnn\n",
    "samples = [generate_sample(' Al') for i in tqdm_utils.tqdm_notebook_failsafe(range(25))]\n",
    "submission = (history, samples)\n",
    "submit_char_rnn(submission, COURSERA_EMAIL, COURSERA_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try it out!\n",
    "\n",
    "__Disclaimer:__ This part of assignment is entirely optional. You won't receive bonus points for it. However, it's a fun thing to do. Please share your results on course forums.\n",
    "\n",
    "You've just implemented a recurrent language model that can be tasked with generating any kind of sequence, so there's plenty of data you can try it on:\n",
    "\n",
    "* Novels/poems/songs of your favorite author\n",
    "* News titles/clickbait titles\n",
    "* Source code of Linux or Tensorflow\n",
    "* Molecules in [smiles](https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system) format\n",
    "* Melody in notes/chords format\n",
    "* IKEA catalog titles\n",
    "* Pokemon names\n",
    "* Cards from Magic, the Gathering / Hearthstone\n",
    "\n",
    "If you're willing to give it a try, here's what you wanna look at:\n",
    "* Current data format is a sequence of lines, so a novel can be formatted as a list of sentences. Alternatively, you can change data preprocessing altogether.\n",
    "* While some datasets are readily available, others can only be scraped from the web. Try `Selenium` or `Scrapy` for that.\n",
    "* Make sure MAX_LENGTH is adjusted for longer datasets. There's also a bonus section about dynamic RNNs at the bottom.\n",
    "* More complex tasks require larger RNN architecture, try more neurons or several layers. It would also require more training iterations.\n",
    "* Long-term dependencies in music, novels or molecules are better handled with LSTM or GRU\n",
    "\n",
    "__Good hunting!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Bonus level: dynamic RNNs\n",
    "\n",
    "Apart from Keras, there's also a friendly TensorFlow API for recurrent neural nets. It's based around the symbolic loop function (aka [tf.scan](https://www.tensorflow.org/api_docs/python/tf/scan)).\n",
    "\n",
    "RNN loop that we implemented for training can be replaced with single TensorFlow instruction: [tf.nn.dynamic_rnn](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn).\n",
    "This interface allows for dynamic sequence length and comes with some pre-implemented architectures.\n",
    "\n",
    "Take a look at [tf.nn.rnn_cell.BasicRNNCell](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicRNNCell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:27:12.975354Z",
     "start_time": "2018-08-13T20:27:12.737529Z"
    }
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Variable rnn/custom_rnn/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\Users\\laloh\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1949, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"C:\\Users\\laloh\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3477, in _create_op_internal\n    ret = Operation(\n  File \"C:\\Users\\laloh\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 742, in _apply_op_helper\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n  File \"C:\\Users\\laloh\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\", line 1752, in variable_v2\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n  File \"C:\\Users\\laloh\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\state_ops.py\", line 74, in variable_op_v2\n    return gen_state_ops.variable_v2(\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-6fd7cf6e318b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0minput_sequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mpredicted_probas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdynamic_rnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_sequence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'LSTM outputs for each step [batch,time,n_tokens]:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m               instructions)\n\u001b[1;32m--> 324\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[1;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[0;32m    682\u001b[0m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 684\u001b[1;33m     (outputs, final_state) = _dynamic_rnn_loop(\n\u001b[0m\u001b[0;32m    685\u001b[0m         \u001b[0mcell\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m_dynamic_rnn_loop\u001b[1;34m(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\u001b[0m\n\u001b[0;32m    886\u001b[0m     \u001b[0mloop_bound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime_steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 888\u001b[1;33m   _, output_final_ta, final_state = control_flow_ops.while_loop(\n\u001b[0m\u001b[0;32m    889\u001b[0m       \u001b[0mcond\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtime\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mloop_bound\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m       \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_time_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[0;32m   2771\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mouter_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2772\u001b[0m       \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWHILE_CONTEXT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2773\u001b[1;33m     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants,\n\u001b[0m\u001b[0;32m   2774\u001b[0m                                     return_same_structure)\n\u001b[0;32m   2775\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmaximum_iterations\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mBuildLoop\u001b[1;34m(self, pred, body, loop_vars, shape_invariants, return_same_structure)\u001b[0m\n\u001b[0;32m   2253\u001b[0m       \u001b[1;31m# new ops.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2254\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2255\u001b[1;33m         original_body_result, exit_vars = self._BuildLoop(\n\u001b[0m\u001b[0;32m   2256\u001b[0m             pred, body, original_loop_vars, loop_vars, shape_invariants)\n\u001b[0;32m   2257\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36m_BuildLoop\u001b[1;34m(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\u001b[0m\n\u001b[0;32m   2179\u001b[0m         expand_composites=True)\n\u001b[0;32m   2180\u001b[0m     \u001b[0mpre_summaries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2181\u001b[1;33m     \u001b[0mbody_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpacked_vars_for_body\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2182\u001b[0m     \u001b[0mpost_summaries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2183\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_sequence_or_composite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(i, lv)\u001b[0m\n\u001b[0;32m   2724\u001b[0m         cond = lambda i, lv: (  # pylint: disable=g-long-lambda\n\u001b[0;32m   2725\u001b[0m             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))\n\u001b[1;32m-> 2726\u001b[1;33m         \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morig_body\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2727\u001b[0m       \u001b[0mtry_to_pack\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m_time_step\u001b[1;34m(time, output_ta_t, state)\u001b[0m\n\u001b[0;32m    863\u001b[0m           skip_conditionals=True)\n\u001b[0;32m    864\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 865\u001b[1;33m       \u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m     \u001b[1;31m# Pack state if using state tuples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    849\u001b[0m     \u001b[0minput_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m     \u001b[1;31m# Keras RNN cells only accept state as list, even if it's a single tensor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 851\u001b[1;33m     \u001b[0mcall_cell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    853\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, state, scope, *args, **kwargs)\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[1;31m# Instead, it is up to subclasses to provide a proper build\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[1;31m# method.  See the class docstring for more details.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m     return base_layer.Layer.__call__(\n\u001b[0m\u001b[0;32m    386\u001b[0m         self, inputs, state, scope=scope, *args, **kwargs)\n\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\legacy_tf_layers\\base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m       \u001b[1;31m# Actually call layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    755\u001b[0m           \u001b[1;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m           \u001b[1;31m# overridden).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 757\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    758\u001b[0m           \u001b[0mcast_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2096\u001b[0m         \u001b[1;31m# operations.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2097\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2098\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2099\u001b[0m       \u001b[1;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2100\u001b[0m       \u001b[1;31m# constrained to set self.built.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(instance, input_shape)\u001b[0m\n\u001b[0;32m    321\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m       \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_shapes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_tuples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m     \u001b[0moutput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m     \u001b[1;31m# Return shapes from `fn` as TensorShapes.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0moutput_shape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, inputs_shape)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[0minput_depth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m     self._kernel = self.add_variable(\n\u001b[0m\u001b[0;32m    455\u001b[0m         \u001b[0m_WEIGHTS_VARIABLE_NAME\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m         shape=[input_depth + self._num_units, self._num_units])\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m               instructions)\n\u001b[1;32m--> 324\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py\u001b[0m in \u001b[0;36madd_variable\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1706\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0madd_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     \u001b[1;34m\"\"\"Deprecated, do NOT use! Alias for `add_weight`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\legacy_tf_layers\\base.py\u001b[0m in \u001b[0;36madd_weight\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, use_resource, synchronization, aggregation, partitioner, **kwargs)\u001b[0m\n\u001b[0;32m    446\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minitializer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m           \u001b[0minitializer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscope\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m         variable = super(Layer, self).add_weight(\n\u001b[0m\u001b[0;32m    449\u001b[0m             \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m             \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py\u001b[0m in \u001b[0;36madd_weight\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[0;32m    429\u001b[0m         \u001b[0mcaching_device\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 431\u001b[1;33m     variable = self._add_variable_with_custom_getter(\n\u001b[0m\u001b[0;32m    432\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[1;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[0;32m    743\u001b[0m         \u001b[0minitializer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheckpoint_initializer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    744\u001b[0m         \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 745\u001b[1;33m     new_variable = getter(\n\u001b[0m\u001b[0;32m    746\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    747\u001b[0m         \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1554\u001b[0m                  \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mVariableSynchronization\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAUTO\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1555\u001b[0m                  aggregation=VariableAggregation.NONE):\n\u001b[1;32m-> 1556\u001b[1;33m   return get_variable_scope().get_variable(\n\u001b[0m\u001b[0;32m   1557\u001b[0m       \u001b[0m_get_default_variable_store\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1558\u001b[0m       \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1297\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1298\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1299\u001b[1;33m       return var_store.get_variable(\n\u001b[0m\u001b[0;32m   1300\u001b[0m           \u001b[0mfull_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1301\u001b[0m           \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    552\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mcustom_getter_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 554\u001b[1;33m       return _true_getter(\n\u001b[0m\u001b[0;32m    555\u001b[0m           \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m           \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    505\u001b[0m             \"name was already created with partitioning?\" % name)\n\u001b[0;32m    506\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 507\u001b[1;33m       return self._get_single_variable(\n\u001b[0m\u001b[0;32m    508\u001b[0m           \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m           \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    868\u001b[0m         \u001b[1;31m# default case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m         \u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34m\"tensorflow/python\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 870\u001b[1;33m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" %\n\u001b[0m\u001b[0;32m    871\u001b[0m                          (err_msg, \"\".join(traceback.format_list(tb))))\n\u001b[0;32m    872\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable rnn/custom_rnn/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\Users\\laloh\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1949, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"C:\\Users\\laloh\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3477, in _create_op_internal\n    ret = Operation(\n  File \"C:\\Users\\laloh\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 742, in _apply_op_helper\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n  File \"C:\\Users\\laloh\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\", line 1752, in variable_v2\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n  File \"C:\\Users\\laloh\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\state_ops.py\", line 74, in variable_op_v2\n    return gen_state_ops.variable_v2(\n"
     ]
    }
   ],
   "source": [
    "class CustomRNN(tf.nn.rnn_cell.BasicRNNCell):\n",
    "    def call(self, input, state):\n",
    "        # from docs:\n",
    "        # Returns:\n",
    "        # Output: A 2-D tensor with shape [batch_size, self.output_size].\n",
    "        # New state: Either a single 2-D tensor, or a tuple of tensors matching the arity and shapes of state.\n",
    "        return rnn_one_step(input[:, 0], state)\n",
    "    \n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return n_tokens\n",
    "\n",
    "cell = CustomRNN(rnn_num_units)\n",
    "\n",
    "input_sequence = tf.placeholder(tf.float32, (None, None))\n",
    "    \n",
    "predicted_probas, last_state = tf.nn.dynamic_rnn(cell, input_sequence[:, :, None], dtype=tf.float32)\n",
    "\n",
    "print('LSTM outputs for each step [batch,time,n_tokens]:')\n",
    "print(predicted_probas({input_sequence: to_matrix(names[:10], max_len=50)}).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we never used MAX_LENGTH in the code above: TF will iterate over however many time-steps you gave it.\n",
    "\n",
    "You can also use any pre-implemented RNN cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:27:12.981697Z",
     "start_time": "2018-08-13T20:27:12.977590Z"
    }
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.compat.v1' has no attribute 'contrib'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-108-e87fcba55e34>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn_cell\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cell'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.compat.v1' has no attribute 'contrib'"
     ]
    }
   ],
   "source": [
    "for obj in dir(tf.nn.rnn_cell) + dir(tf.contrib.rnn):\n",
    "    if obj.endswith('Cell'):\n",
    "        print(obj, end=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:27:13.168207Z",
     "start_time": "2018-08-13T20:27:12.986884Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From <ipython-input-109-62766a2145fb>:6: LSTMCell.__init__ (from tensorflow.python.keras.layers.legacy_rnn.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "LSTM hidden state for each step [batch,time,rnn_num_units]:\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-109-62766a2145fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'LSTM hidden state for each step [batch,time,rnn_num_units]:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_sequence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0minput_sequence\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mto_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m    911\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m     \"\"\"\n\u001b[1;32m--> 913\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mdeprecation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Use ref() instead.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[1;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   5496\u001b[0m     \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5497\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msession\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5498\u001b[1;33m       raise ValueError(\"Cannot evaluate tensor using `eval()`: No default \"\n\u001b[0m\u001b[0;32m   5499\u001b[0m                        \u001b[1;34m\"session is registered. Use `with \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5500\u001b[0m                        \u001b[1;34m\"sess.as_default()` or pass an explicit session to \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`"
     ]
    }
   ],
   "source": [
    "input_sequence = tf.placeholder(tf.int32, (None, None))\n",
    "\n",
    "inputs_embedded = embed_x(input_sequence)\n",
    "\n",
    "# standard cell returns hidden state as output!\n",
    "cell = tf.nn.rnn_cell.LSTMCell(rnn_num_units)\n",
    "\n",
    "state_sequence, last_state = tf.nn.dynamic_rnn(cell, inputs_embedded, dtype=tf.float32)\n",
    "\n",
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "print('LSTM hidden state for each step [batch,time,rnn_num_units]:')\n",
    "print(state_sequence.eval({input_sequence: to_matrix(names[:10], max_len=50)}).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}